{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, StringIndexer\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rseed = 1024\n",
    "random.seed(rseed)\n",
    "\n",
    "start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def base_features_gen_pipeline(input_descript_col=\"descript\", input_category_col=\"category\", output_feature_col=\"features\", output_label_col=\"label\"):\n",
    "    token = Tokenizer(inputCol=input_descript_col, outputCol=\"words\")\n",
    "    cv = CountVectorizer(inputCol=\"words\", outputCol=output_feature_col)\n",
    "    index = StringIndexer(inputCol=input_category_col, outputCol=output_label_col)\n",
    "    return Pipeline(stages=[token, cv, index])\n",
    "\n",
    "def gen_meta_features(training_df, nb_0, nb_1, nb_2, svm_0, svm_1, svm_2):\n",
    "    k_fold = training_df.select(\"group\").distinct().count()\n",
    "    flag = 0\n",
    "    for k in range(0,k_fold):\n",
    "        train_group = training_df.filter(training_df[\"group\"]!=k) \n",
    "        test_group = training_df.filter(training_df[\"group\"]==k) \n",
    "        \n",
    "        pipe = Pipeline(stages=[nb_0, nb_1, nb_2, svm_0, svm_1, svm_2])\n",
    "        model = pipe.fit(train_group)\n",
    "        \n",
    "        if flag==0:\n",
    "            result = model.transform(test_group)\n",
    "            flag = flag+1\n",
    "        else:\n",
    "            temp = model.transform(test_group)\n",
    "            result = result.union(temp)\n",
    "    \n",
    "    result = result.withColumn(\"joint_pred_0\", 2*col(\"nb_pred_0\")+col(\"svm_pred_0\"))\n",
    "    result = result.withColumn(\"joint_pred_1\", 2*col(\"nb_pred_1\")+col(\"svm_pred_1\"))\n",
    "    result = result.withColumn(\"joint_pred_2\", 2*col(\"nb_pred_2\")+col(\"svm_pred_2\"))\n",
    "#     result = result.withColumn(\"joint_pred_0\", (joint(\"nb_pred_0\", \"svm_pred_0\")).cast(DoubleType()))\n",
    "#     result = result.withColumn(\"joint_pred_1\", (joint(\"nb_pred_1\", \"svm_pred_1\")).cast(DoubleType()))\n",
    "#     result = result.withColumn(\"joint_pred_2\", (joint(\"nb_pred_2\", \"svm_pred_2\")).cast(DoubleType()))\n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "def test_prediction(test_df, base_features_pipeline_model, gen_base_pred_pipeline_model, gen_meta_feature_pipeline_model, meta_classifier):\n",
    "    base_feature = base_features_pipeline_model.transform(test_df)\n",
    "    feature_model = gen_base_pred_pipeline_model.transform(base_feature)\n",
    "    feature_join = feature_model.withColumn(\"joint_pred_0\", 2*col(\"nb_pred_0\")+col(\"svm_pred_0\"))\n",
    "    feature_join = feature_join.withColumn(\"joint_pred_1\", 2*col(\"nb_pred_1\")+col(\"svm_pred_1\"))\n",
    "    feature_join = feature_join.withColumn(\"joint_pred_2\", 2*col(\"nb_pred_2\")+col(\"svm_pred_2\"))\n",
    "    final_feature = gen_meta_feature_pipeline_model.transform(feature_join)\n",
    "    pred = meta_classifier.transform(final_feature).select(\"id\", \"label\", \"final_prediction\")\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_binary_labels(df):\n",
    "    df = df.withColumn('label_0', (df['label'] == 0).cast(DoubleType()))\n",
    "    df = df.withColumn('label_1', (df['label'] == 1).cast(DoubleType()))\n",
    "    df = df.withColumn('label_2', (df['label'] == 2).cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "# Create a Spark Session\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"lab3\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Load data\n",
    "train_data = spark.read.load(\"proj2train.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "test_data = spark.read.load(\"proj2test.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n",
      "+---+--------------------+-----+\n",
      "| id|            features|label|\n",
      "+---+--------------------+-----+\n",
      "|  0|(5421,[1,18,31,39...|  1.0|\n",
      "|  1|(5421,[0,1,15,20,...|  0.0|\n",
      "|  2|(5421,[3,109,556,...|  0.0|\n",
      "|  3|(5421,[1,2,3,5,6,...|  1.0|\n",
      "|  4|(5421,[2,3,4,8,11...|  1.0|\n",
      "+---+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the pipeline from task 1.1\n",
    "base_features_pipeline = base_features_gen_pipeline()\n",
    "# Fit the pipeline using train_data\n",
    "base_features_pipeline_model = base_features_pipeline.fit(train_data)\n",
    "# Transform the train_data using fitted pipeline\n",
    "training_set = base_features_pipeline_model.transform(train_data)\n",
    "\n",
    "training_set.printSchema()\n",
    "training_set.select(\"id\", \"features\", \"label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- group: integer (nullable = true)\n",
      " |-- label_0: double (nullable = false)\n",
      " |-- label_1: double (nullable = false)\n",
      " |-- label_2: double (nullable = false)\n",
      "\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+\n",
      "| id|category|            descript|               words|            features|label|group|label_0|label_1|label_2|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+\n",
      "|  0|    MISC|I've been there t...|[i've, been, ther...|(5421,[1,18,31,39...|  1.0|    4|    0.0|    1.0|    0.0|\n",
      "|  1|    FOOD|Stay away from th...|[stay, away, from...|(5421,[0,1,15,20,...|  0.0|    4|    1.0|    0.0|    0.0|\n",
      "|  2|    FOOD|Wow over 100 beer...|[wow, over, 100, ...|(5421,[3,109,556,...|  0.0|    4|    1.0|    0.0|    0.0|\n",
      "|  3|    MISC|Having been a lon...|[having, been, a,...|(5421,[1,2,3,5,6,...|  1.0|    0|    0.0|    1.0|    0.0|\n",
      "|  4|    MISC|This is a consist...|[this, is, a, con...|(5421,[2,3,4,8,11...|  1.0|    2|    0.0|    1.0|    0.0|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign random groups and binarize the labels\n",
    "training_set = training_set.withColumn('group', (rand(rseed)*5).cast(IntegerType()))\n",
    "training_set = gen_binary_labels(training_set)\n",
    "\n",
    "training_set.printSchema()\n",
    "training_set.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base models\n",
    "nb_0 = NaiveBayes(featuresCol='features', labelCol='label_0', predictionCol='nb_pred_0', probabilityCol='nb_prob_0', rawPredictionCol='nb_raw_0')\n",
    "nb_1 = NaiveBayes(featuresCol='features', labelCol='label_1', predictionCol='nb_pred_1', probabilityCol='nb_prob_1', rawPredictionCol='nb_raw_1')\n",
    "nb_2 = NaiveBayes(featuresCol='features', labelCol='label_2', predictionCol='nb_pred_2', probabilityCol='nb_prob_2', rawPredictionCol='nb_raw_2')\n",
    "svm_0 = LinearSVC(featuresCol='features', labelCol='label_0', predictionCol='svm_pred_0', rawPredictionCol='svm_raw_0')\n",
    "svm_1 = LinearSVC(featuresCol='features', labelCol='label_1', predictionCol='svm_pred_1', rawPredictionCol='svm_raw_1')\n",
    "svm_2 = LinearSVC(featuresCol='features', labelCol='label_2', predictionCol='svm_pred_2', rawPredictionCol='svm_raw_2')\n",
    "\n",
    "# build pipeline to generate predictions from base classifiers, will be used in task 1.3\n",
    "gen_base_pred_pipeline = Pipeline(stages=[nb_0, nb_1, nb_2, svm_0, svm_1, svm_2])\n",
    "gen_base_pred_pipeline_model = gen_base_pred_pipeline.fit(training_set)\n",
    "\n",
    "# task 1.2\n",
    "meta_features = gen_meta_features(training_set, nb_0, nb_1, nb_2, svm_0, svm_1, svm_2)\n",
    "# meta_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of pred_test : \n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- final_prediction: double (nullable = false)\n",
      "\n",
      "total running time:  162.86369013786316\n",
      "evaluate score of testing data is: \n",
      "0.7483312619309965\n"
     ]
    }
   ],
   "source": [
    "# build onehotencoder and vectorassembler pipeline\n",
    "onehot_encoder = OneHotEncoderEstimator(inputCols=['nb_pred_0', 'nb_pred_1', 'nb_pred_2', 'svm_pred_0', 'svm_pred_1', 'svm_pred_2', 'joint_pred_0', 'joint_pred_1', 'joint_pred_2'], outputCols=['vec{}'.format(i) for i in range(9)])\n",
    "vector_assembler = VectorAssembler(inputCols=['vec{}'.format(i) for i in range(9)], outputCol='meta_features')\n",
    "gen_meta_feature_pipeline = Pipeline(stages=[onehot_encoder, vector_assembler])\n",
    "gen_meta_feature_pipeline_model = gen_meta_feature_pipeline.fit(meta_features)\n",
    "meta_features = gen_meta_feature_pipeline_model.transform(meta_features)\n",
    "\n",
    "# train the meta clasifier\n",
    "lr_model = LogisticRegression(featuresCol='meta_features', labelCol='label', predictionCol='final_prediction', maxIter=20, regParam=1., elasticNetParam=0)\n",
    "meta_classifier = lr_model.fit(meta_features)\n",
    "\n",
    "# task 1.3\n",
    "pred_test = test_prediction(test_data, base_features_pipeline_model, gen_base_pred_pipeline_model, gen_meta_feature_pipeline_model, meta_classifier)\n",
    "print(\"Schema of pred_test : \")\n",
    "pred_test.printSchema()\n",
    "end_time = time.time()\n",
    "print(\"total running time: \", end_time-start_time)\n",
    "# Evaluation\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",metricName='f1')\n",
    "print(\"evaluate score of testing data is: \")\n",
    "print(evaluator.evaluate(pred_test, {evaluator.predictionCol:'final_prediction'}))\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP9313",
   "language": "python",
   "name": "comp9313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
